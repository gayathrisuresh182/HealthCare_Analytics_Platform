# Project Structure - Healthcare Analytics Platform

## ğŸ“ **Directory Structure**

```
HealthCare_Analytics_Platform/
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ dbt_ci.yml              # CI/CD pipeline
â”‚
â”œâ”€â”€ analyses/                        # Ad-hoc analysis queries
â”‚   â”œâ”€â”€ cost_quality_correlation.sql
â”‚   â”œâ”€â”€ geographic_efficiency.sql
â”‚   â””â”€â”€ ownership_impact_analysis.sql
â”‚
â”œâ”€â”€ data/                           # Source CSV files (local)
â”‚   â”œâ”€â”€ hospital_general_info.csv
â”‚   â”œâ”€â”€ ipps_charges.csv
â”‚   â””â”€â”€ readmissions.csv
â”‚
â”œâ”€â”€ docs/                           # Documentation
â”‚   â”œâ”€â”€ architecture_overview.md
â”‚   â”œâ”€â”€ data_lake_structure.md
â”‚   â”œâ”€â”€ project_structure.md (this file)
â”‚   â”œâ”€â”€ comprehensive_data_analytics_guide.md
â”‚   â”œâ”€â”€ great_expectations_implementation.md
â”‚   â”œâ”€â”€ pipeline_automation_guide.md
â”‚   â”œâ”€â”€ project_completion_summary.md
â”‚   â”œâ”€â”€ snapshots_guide.md
â”‚   â””â”€â”€ tableau_setup_guide.md
â”‚
â”œâ”€â”€ dbt_packages/                   # dbt package dependencies
â”‚   â””â”€â”€ dbt_utils/                  # Utility macros
â”‚
â”œâ”€â”€ gx/                             # Great Expectations
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”‚   â””â”€â”€ marts_checkpoint.yml
â”‚   â”œâ”€â”€ expectations/
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â”œâ”€â”€ great_expectations.yml
â”‚   â””â”€â”€ uncommitted/
â”‚       â””â”€â”€ data_docs/
â”‚
â”œâ”€â”€ logs/                           # dbt logs
â”‚   â””â”€â”€ dbt.log
â”‚
â”œâ”€â”€ macros/                         # Reusable dbt macros
â”‚   â””â”€â”€ custom_tests.sql
â”‚
â”œâ”€â”€ models/                         # dbt models
â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â”œâ”€â”€ int_charges_quality_merged.sql
â”‚   â”‚   â”œâ”€â”€ int_hospital_cost_metrics.sql
â”‚   â”‚   â”œâ”€â”€ int_readmission_analysis.sql
â”‚   â”‚   â””â”€â”€ schema.yml
â”‚   â”œâ”€â”€ marts/
â”‚   â”‚   â”œâ”€â”€ dim_dates.sql
â”‚   â”‚   â”œâ”€â”€ dim_drg_codes.sql
â”‚   â”‚   â”œâ”€â”€ dim_geography.sql
â”‚   â”‚   â”œâ”€â”€ dim_hospitals.sql
â”‚   â”‚   â”œâ”€â”€ fct_hospital_summary.sql
â”‚   â”‚   â”œâ”€â”€ fct_inpatient_charges.sql
â”‚   â”‚   â”œâ”€â”€ fct_readmissions.sql
â”‚   â”‚   â”œâ”€â”€ fct_state_summary.sql
â”‚   â”‚   â””â”€â”€ schema.yml
â”‚   â””â”€â”€ staging/
â”‚       â”œâ”€â”€ schema.yml
â”‚       â”œâ”€â”€ sources.yml
â”‚       â”œâ”€â”€ stg_hospitals.sql
â”‚       â”œâ”€â”€ stg_ipps_charges.sql
â”‚       â””â”€â”€ stg_readmissions.sql
â”‚
â”œâ”€â”€ scripts/                        # Automation and utility scripts
â”‚   â”œâ”€â”€ gx_*.py                     # Great Expectations scripts
â”‚   â”œâ”€â”€ run_pipeline*.py            # Pipeline automation
â”‚   â”œâ”€â”€ tableau_custom_sql_*.sql    # Tableau queries
â”‚   â””â”€â”€ *.sql                       # SQL utilities
â”‚
â”œâ”€â”€ snapshots/                      # SCD Type 2 snapshots
â”‚   â””â”€â”€ hospitals_snapshot.sql
â”‚
â”œâ”€â”€ target/                         # dbt compilation output
â”‚
â”œâ”€â”€ tests/                          # Custom data tests
â”‚   â”œâ”€â”€ assert_business_rules.sql
â”‚   â”œâ”€â”€ assert_positive_charges.sql
â”‚   â”œâ”€â”€ assert_readmission_ratio_valid.sql
â”‚   â””â”€â”€ assert_valid_ratings.sql
â”‚
â”œâ”€â”€ .gitignore                      # Git ignore rules
â”œâ”€â”€ dbt_project.yml                 # dbt project configuration
â”œâ”€â”€ packages.yml                    # dbt package dependencies
â”œâ”€â”€ profiles.yml.template          # dbt connection template
â”œâ”€â”€ QUICK_START.md                  # Quick start guide
â”œâ”€â”€ README_GX_ENHANCEMENTS.md       # GX enhancements guide
â””â”€â”€ Readme.md                       # Main project README
```

---

## ğŸ“‚ **Directory Purposes**

### **`.github/workflows/`**
- **Purpose**: CI/CD pipeline definitions
- **Files**: GitHub Actions workflows
- **Key File**: `dbt_ci.yml` - Automated testing and deployment

### **`analyses/`**
- **Purpose**: Ad-hoc analysis queries (not part of dbt pipeline)
- **Use Case**: Exploratory analysis, one-off queries
- **Files**: SQL analysis queries

### **`data/`**
- **Purpose**: Local source CSV files
- **Note**: These are loaded to S3 and Snowflake
- **Files**: 3 CSV files (charges, hospitals, readmissions)

### **`docs/`**
- **Purpose**: Project documentation
- **Contents**: Architecture, setup guides, best practices
- **Key Files**: Architecture overview, data lake structure, setup guides

### **`gx/`**
- **Purpose**: Great Expectations configuration
- **Contents**: Expectation suites, checkpoints, data docs
- **Key Files**: `great_expectations.yml`, checkpoints, expectations

### **`macros/`**
- **Purpose**: Reusable dbt macros
- **Files**: Custom test macros, utility functions

### **`models/`**
- **Purpose**: dbt transformation models
- **Structure**:
  - `staging/`: Clean and standardize (views)
  - `intermediate/`: Business logic (views)
  - `marts/`: Dimensional model (tables)
- **Files**: SQL models + schema.yml for tests

### **`scripts/`**
- **Purpose**: Automation and utility scripts
- **Categories**:
  - Great Expectations scripts (`gx_*.py`)
  - Pipeline automation (`run_pipeline*.py`)
  - Tableau SQL queries (`tableau_custom_sql_*.sql`)
  - SQL utilities

### **`snapshots/`**
- **Purpose**: SCD Type 2 historical tracking
- **Files**: Snapshot definitions

### **`tests/`**
- **Purpose**: Custom data quality tests
- **Files**: SQL test queries

---

## ğŸ”§ **Configuration Files**

### **`dbt_project.yml`**
- **Purpose**: dbt project configuration
- **Contents**: 
  - Model materialization (views vs tables)
  - Schema mappings
  - Variables
  - Package dependencies

### **`packages.yml`**
- **Purpose**: dbt package dependencies
- **Contents**: `dbt_utils` package

### **`profiles.yml.template`**
- **Purpose**: Template for Snowflake connection
- **Usage**: Copy to `~/.dbt/profiles.yml` and fill in credentials

### **`.gitignore`**
- **Purpose**: Exclude files from Git
- **Contents**: `target/`, `logs/`, `dbt_packages/`, credentials

---

## ğŸ¯ **Model Organization**

### **Staging Models** (`models/staging/`)
- **Naming**: `stg_*`
- **Purpose**: Clean and standardize raw data
- **Materialization**: Views
- **Schema**: `raw_staging`

### **Intermediate Models** (`models/intermediate/`)
- **Naming**: `int_*`
- **Purpose**: Business logic, calculations, joins
- **Materialization**: Views
- **Schema**: `raw_intermediate`

### **Mart Models** (`models/marts/`)
- **Naming**: `dim_*` (dimensions), `fct_*` (facts)
- **Purpose**: Analytics-ready dimensional model
- **Materialization**: Tables
- **Schema**: `raw_marts`

---

## ğŸ“Š **File Naming Conventions**

### **Models:**
- **Staging**: `stg_<source_name>.sql`
- **Intermediate**: `int_<purpose>.sql`
- **Dimensions**: `dim_<entity>.sql`
- **Facts**: `fct_<entity>.sql`

### **Tests:**
- **Custom tests**: `assert_<rule>.sql`
- **Schema tests**: Defined in `schema.yml`

### **Scripts:**
- **Great Expectations**: `gx_<action>.py`
- **Pipeline**: `run_pipeline*.py` or `.sh` or `.bat`
- **SQL utilities**: `<purpose>.sql`

---

## ğŸ”— **Dependencies**

### **dbt Packages:**
- `dbt_utils` - Utility macros for tests and transformations

### **Python Packages:**
- `dbt-snowflake` - dbt adapter for Snowflake
- `great-expectations` - Data quality framework
- `snowflake-sqlalchemy` - SQLAlchemy driver for Snowflake

---

## ğŸ“ **Documentation Files**

### **Essential Documentation:**
- `docs/architecture_overview.md` - System architecture
- `docs/data_lake_structure.md` - Data lake organization
- `docs/project_structure.md` - This file

---

## ğŸš€ **Workflow**

### **Development:**
1. Edit models in `models/`
2. Run `dbt run` to test
3. Run `dbt test` to validate
4. Commit to Git

### **CI/CD:**
1. Push to GitHub
2. GitHub Actions runs tests
3. On `main` branch â†’ Production deployment
4. On `develop` branch â†’ Dev deployment

### **Data Quality:**
1. Run `dbt test` for schema tests
2. Run `python scripts/gx_run_checkpoint.py` for GX validation
3. Review data docs

---

This structure supports maintainable, scalable, and production-ready data engineering.

